# Transaction Data Integration (ETL)

This project is a take-home assignment that demonstrates the design and implementation of an **ETL pipeline in Python**.  
The goal is to ingest transaction data from multiple sources (**CSV, JSON, XML**), validate and clean it, standardize into a consistent schema, convert amounts to USD, and store results for analysis.

---

## ğŸ“‚ Project Structure

```
txn_integration/
â”œâ”€â”€ main.py                 # Orchestrator script
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ ingest.py           # Load and unify CSV, JSON, XML
â”‚   â”œâ”€â”€ transform.py        # Normalize, dedupe, suspicious flags
â”‚   â”œâ”€â”€ validate.py         # Required fields & invalid record handling
â”‚   â”œâ”€â”€ currency.py         # Currency conversion (Frankfurter API)
â”‚   â””â”€â”€ storage.py          # Save outputs to CSV + SQLite
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_transform.py   # Sample pytest tests
â”œâ”€â”€ requirements.txt        # Python dependencies
â””â”€â”€ README.md               # Documentation
```

---

## ğŸš€ Setup & Run

1. Clone the repo or unzip the project:

   ```bash
   git clone https://github.com/<your-username>/transaction-data-integration.git
   cd transaction-data-integration
   ```

2. Create and activate a virtual environment:

   ```bash
   python -m venv .venv
   source .venv/bin/activate   # Mac/Linux
   .venv\Scripts\activate      # Windows
   ```

3. Install dependencies:

   ```bash
   pip install -r requirements.txt
   ```

4. Place input files into a `data/` folder:

   - `transactions_storeA.csv`
   - `transactions_online.json`
   - `transactions_partner.xml`

5. Run the pipeline:

   ```bash
   python main.py --input-dir data --out-dir out
   ```

6. Optional: provide a fallback date for missing timestamps:

   ```bash
   python main.py --input-dir data --out-dir out --default-date 2025-01-01
   ```

7. Run tests:
   ```bash
   pytest -q
   ```

---

## ğŸ“Š Target Schema

| Column            | Description                                        |
| ----------------- | -------------------------------------------------- |
| `transaction_id`  | Unique identifier for the transaction              |
| `source`          | Source system/file (e.g., storeA, online, partner) |
| `timestamp`       | Transaction timestamp (ISO format)                 |
| `customer_id`     | Unique customer identifier                         |
| `amount_original` | Original transaction amount                        |
| `currency`        | Original currency                                  |
| `amount_usd`      | Converted amount in USD (via Frankfurter API)      |
| `payment_method`  | Payment channel/method                             |
| `status`          | Transaction status (if provided)                   |

---

## ğŸ“‚ Outputs

- `clean_transactions.csv` â†’ standardized, validated dataset (with `amount_usd`)
- `errors.csv` â†’ invalid rows (with reasons)
- `suspicious.csv` â†’ flagged negative or unusually large amounts
- `transactions.sqlite` â†’ SQLite DB with `transactions` table

---

## ğŸ”‘ Key Features

- **Ingestion** â†’ CSV (pandas), JSON (json), XML (ElementTree)
- **Normalization** â†’ consistent schema for all sources
- **Validation** â†’ required field checks, invalid rows quarantined
- **Deduplication** â†’ drops duplicate `transaction_id`s
- **Currency Conversion** â†’ Frankfurter API (historical by date, cached)
- **Suspicious Detection** â†’ negative & abnormally large amounts
- **Storage** â†’ outputs to CSV + SQLite
- **Testing** â†’ pytest unit tests for transform/validation logic

---

## ğŸ§© Assumptions & Design Decisions

- Schema-first design keeps the pipeline simple and predictable.
- Invalid rows are logged to `errors.csv` instead of breaking the run.
- Frankfurter API provides reliable FX rates (historical when date is known).
- SQLite chosen for portability; easily replaced by MySQL/Postgres in production.
- Suspicious rules are intentionally simple (negative / huge amounts).
- Modular file structure mirrors real ETL pipelines and is easy to extend.

---

## ğŸ”„ ETL Flow

```
CSV / JSON / XML
       â†“
   Ingest (etl/ingest.py)
       â†“
   Normalize & Clean (etl/transform.py)
       â†“
   Validate & Split (etl/validate.py)
       â†“
   Deduplicate
       â†“
   Currency Conversion (etl/currency.py)
       â†“
   Flag Suspicious
       â†“
   Store â†’ clean_transactions.csv / errors.csv / suspicious.csv / transactions.sqlite
```

---

## âš ï¸ Disclaimer

This solution is for **demonstration purposes only**. Deduction rates, suspicious thresholds, and some assumptions are illustrative. In production, these would be refined based on business rules, compliance, and performance requirements.
